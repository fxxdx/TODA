{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from pytorch_lightning.metrics.functional import  accuracy \n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAR_torch_Dataset(Dataset):\n",
    "    def __init__(self, data,labels, transform= None):\n",
    "        \"\"\"Reads source and target sequences from processing file .\"\"\"        \n",
    "        self.input_tensor = (torch.from_numpy(data)).float()\n",
    "        self.label = torch.LongTensor(labels)\n",
    "        self.transform =transform\n",
    "        self.num_total_seqs = len(self.input_tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (source and target).\"\"\"\n",
    "        input_seq = self.input_tensor[index]\n",
    "        input_labels = self.label[index]\n",
    "        if self.transform:\n",
    "            input_seq = self.transform(input_seq)\n",
    "        return input_seq, input_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_total_seqs\n",
    "\n",
    "def HAR_data_generator(data_dir,save=False):\n",
    "    # dataloading \n",
    "    subject_data= np.loadtxt(f'{data_dir}/train/subject_train.txt')\n",
    "    # Samples\n",
    "    train_acc_x= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_x_train.txt')\n",
    "    train_acc_y= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_y_train.txt')\n",
    "    train_acc_z= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_z_train.txt')\n",
    "    train_gyro_x= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_x_train.txt')\n",
    "    train_gyro_y= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_y_train.txt')\n",
    "    train_gyro_z= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_z_train.txt')\n",
    "    train_tot_acc_x= np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_x_train.txt')\n",
    "    train_tot_acc_y= np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_y_train.txt')\n",
    "    train_tot_acc_z= np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_z_train.txt')\n",
    "\n",
    "    test_acc_x= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_x_test.txt')\n",
    "    test_acc_y= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_y_test.txt')\n",
    "    test_acc_z= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_z_test.txt')\n",
    "    test_gyro_x= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_x_test.txt')\n",
    "    test_gyro_y= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_y_test.txt')\n",
    "    test_gyro_z= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_z_test.txt')\n",
    "    test_tot_acc_x= np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_x_test.txt')\n",
    "    test_tot_acc_y= np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_y_test.txt')\n",
    "    test_tot_acc_z= np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_z_test.txt')\n",
    "\n",
    "    # Stacking channels together data \n",
    "    train_data= np.stack((train_acc_x,train_acc_y,train_acc_z,\n",
    "                              train_gyro_x,train_gyro_y,train_gyro_z,\n",
    "                              train_tot_acc_x, train_tot_acc_y,train_tot_acc_z),axis=1)\n",
    "    test_data= np.stack((test_acc_x,test_acc_y,test_acc_z,\n",
    "                              test_gyro_x,test_gyro_y,test_gyro_z,\n",
    "                              test_tot_acc_x, test_tot_acc_y,test_tot_acc_z),axis=1)\n",
    "    # labels \n",
    "    train_labels=  np.loadtxt(f'{data_dir}/train/y_train.txt')\n",
    "    train_labels -= np.min(train_labels)\n",
    "    test_labels=  np.loadtxt(f'{data_dir}/test/y_test.txt')\n",
    "    test_labels -= np.min(test_labels)\n",
    "\n",
    "    # different subjects \n",
    "    subject_train= np.loadtxt(f'{data_dir}/train/subject_train.txt')\n",
    "    subject_test= np.loadtxt(f'{data_dir}/test/subject_test.txt')\n",
    "    # select subset i for train and subset j for testing\n",
    "\n",
    "    all_subjects_data = np.concatenate((train_data, test_data))\n",
    "    all_subjects_labels = np.concatenate((train_labels, test_labels))\n",
    "    subject_indices =  np.concatenate((subject_train, subject_test))\n",
    "    # arrange the subjects to different domains \n",
    "    for i in range(0, 25, 6):\n",
    "        j= i+6 \n",
    "        domains_data.append(all_subjects_data[np.where((i< subject_indices)&( subject_indices<=j))])\n",
    "        domains_labels.append(all_subjects_labels[np.where((i< subject_indices)&( subject_indices<=j))])\n",
    "\n",
    "    # split the domains to train_val_test\n",
    "    for domain_data, domain_labels, name in zip(domains_data, domains_labels, domain_names):\n",
    "        # train, validation, test split of the data \n",
    "        X_train, X_test, y_train, y_test = train_test_split(domain_data, domain_labels, test_size=0.2, random_state=1)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "        HAR_dataset_processed[name]= {'train':{'samples':X_train, 'labels':y_train},\n",
    "                                   'val':{'samples':X_val, 'labels':y_val},\n",
    "                                   'test':{'samples':X_test, 'labels':y_test}}\n",
    "    if save:\n",
    "        torch.save(HAR_dataset_processed, 'HAR_DG_settings.numpy')\n",
    "    return HAR_dataset_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HAR_data_loader(dataset, batch_size, shuffle, drop_last) :\n",
    "    # datasets \n",
    "    train_dataset = HAR_torch_Dataset(dataset['train']['samples'], dataset['train']['labels'])\n",
    "    val_dataset = HAR_torch_Dataset(dataset['val']['samples'], dataset['val']['labels'])\n",
    "    test_dataset = HAR_torch_Dataset(dataset['test']['samples'], dataset['test']['labels'])\n",
    "    # dataloaders\n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    valid_dl = DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
    "\n",
    "    return train_dl, valid_dl, test_dl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
