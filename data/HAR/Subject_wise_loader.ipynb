{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohamedr002\\Anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from pytorch_lightning.metrics.functional import  accuracy \n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAR_torch_Dataset(Dataset):\n",
    "    def __init__(self, data,labels, transform= None):\n",
    "        \"\"\"Reads source and target sequences from processing file .\"\"\"        \n",
    "        self.input_tensor = (torch.from_numpy(data)).float()\n",
    "        self.label = torch.LongTensor(labels)\n",
    "        self.transform =transform\n",
    "        self.num_total_seqs = len(self.input_tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (source and target).\"\"\"\n",
    "        input_seq = self.input_tensor[index]\n",
    "        input_labels = self.label[index]\n",
    "        if self.transform:\n",
    "            input_seq = self.transform(input_seq)\n",
    "        return input_seq, input_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_total_seqs\n",
    "\n",
    "def HAR_data_generator(data_dir,save=False):\n",
    "    # dataloading \n",
    "    domains_data = []\n",
    "    domains_labels = []\n",
    "    subject_data= np.loadtxt(f'{data_dir}/train/subject_train.txt')\n",
    "    # Samples\n",
    "    train_acc_x= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_x_train.txt')\n",
    "    train_acc_y= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_y_train.txt')\n",
    "    train_acc_z= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_z_train.txt')\n",
    "    train_gyro_x= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_x_train.txt')\n",
    "    train_gyro_y= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_y_train.txt')\n",
    "    train_gyro_z= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_z_train.txt')\n",
    "    train_tot_acc_x= np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_x_train.txt')\n",
    "    train_tot_acc_y= np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_y_train.txt')\n",
    "    train_tot_acc_z= np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_z_train.txt')\n",
    "\n",
    "    test_acc_x= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_x_test.txt')\n",
    "    test_acc_y= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_y_test.txt')\n",
    "    test_acc_z= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_z_test.txt')\n",
    "    test_gyro_x= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_x_test.txt')\n",
    "    test_gyro_y= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_y_test.txt')\n",
    "    test_gyro_z= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_z_test.txt')\n",
    "    test_tot_acc_x= np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_x_test.txt')\n",
    "    test_tot_acc_y= np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_y_test.txt')\n",
    "    test_tot_acc_z= np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_z_test.txt')\n",
    "\n",
    "    # Stacking channels together data \n",
    "    train_data= np.stack((train_acc_x,train_acc_y,train_acc_z,\n",
    "                              train_gyro_x,train_gyro_y,train_gyro_z,\n",
    "                              train_tot_acc_x, train_tot_acc_y,train_tot_acc_z),axis=1)\n",
    "    test_data= np.stack((test_acc_x,test_acc_y,test_acc_z,\n",
    "                              test_gyro_x,test_gyro_y,test_gyro_z,\n",
    "                              test_tot_acc_x, test_tot_acc_y,test_tot_acc_z),axis=1)\n",
    "    # labels \n",
    "    train_labels=  np.loadtxt(f'{data_dir}/train/y_train.txt')\n",
    "    train_labels -= np.min(train_labels)\n",
    "    test_labels=  np.loadtxt(f'{data_dir}/test/y_test.txt')\n",
    "    test_labels -= np.min(test_labels)\n",
    "\n",
    "    # different subjects \n",
    "    subject_train= np.loadtxt(f'{data_dir}/train/subject_train.txt')\n",
    "    subject_test= np.loadtxt(f'{data_dir}/test/subject_test.txt')\n",
    "    # select subset i for train and subset j for testing\n",
    "\n",
    "    all_subjects_data = np.concatenate((train_data, test_data))\n",
    "    all_subjects_labels = np.concatenate((train_labels, test_labels))\n",
    "    subject_indices =  np.concatenate((subject_train, subject_test))\n",
    "    # arrange the subjects to different domains\n",
    "    print(subject_indices)\n",
    "    subject_list = [1,3,5,]\n",
    "    domain_names = list(string.ascii_lowercase)\n",
    "    for i in range(1, 30):\n",
    "        domains_data.append(all_subjects_data[np.where(subject_indices==i)])\n",
    "        domains_labels.append(all_subjects_labels[np.where(subject_indices==i)])\n",
    "\n",
    "    # split the domains to train_val_test\n",
    "    for domain_data, domain_labels, name in zip(domains_data, domains_labels, domain_names):\n",
    "        # train, validation, test split of the data \n",
    "        print(domain_data.shape)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(domain_data, domain_labels, test_size=0.2, random_state=1)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "        train = {'samples':X_train, 'labels':y_train}\n",
    "        val   = {'samples':X_val, 'labels':y_val}\n",
    "        test   = {'samples':X_test, 'labels':y_test}\n",
    "        torch.save(train, f'train_{name}.pt')\n",
    "        torch.save(val,f'val_{name}.pt')\n",
    "        torch.save(test,f'test_{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_names = np.arange(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ... 24. 24. 24.]\n",
      "(347, 9, 128)\n",
      "(302, 9, 128)\n",
      "(341, 9, 128)\n",
      "(317, 9, 128)\n",
      "(302, 9, 128)\n",
      "(325, 9, 128)\n",
      "(308, 9, 128)\n",
      "(281, 9, 128)\n",
      "(288, 9, 128)\n",
      "(294, 9, 128)\n",
      "(316, 9, 128)\n",
      "(320, 9, 128)\n",
      "(327, 9, 128)\n",
      "(323, 9, 128)\n",
      "(328, 9, 128)\n",
      "(366, 9, 128)\n",
      "(368, 9, 128)\n",
      "(364, 9, 128)\n",
      "(360, 9, 128)\n",
      "(354, 9, 128)\n",
      "(408, 9, 128)\n",
      "(321, 9, 128)\n",
      "(372, 9, 128)\n",
      "(381, 9, 128)\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../../UCI HAR Dataset'\n",
    "HAR_data_generator(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_subjects_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7b94c70da8fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_subjects_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'all_subjects_data' is not defined"
     ]
    }
   ],
   "source": [
    "all_subjects_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange the subjects to different domains\n",
    "domain_names = list(string.ascii_lowercase)\n",
    "for i in range(0, 25):\n",
    "    domains_data.append(all_subjects_data[i])\n",
    "    domains_labels.append(all_subjects_labels[i])\n",
    "\n",
    "# split the domains to train_val_test\n",
    "for domain_data, domain_labels, name in zip(domains_data, domains_labels, domain_names):\n",
    "    # train, validation, test split of the data \n",
    "    X_train, X_test, y_train, y_test = train_test_split(domain_data, domain_labels, test_size=0.2, random_state=1)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "    \n",
    "    train = {'samples':X_train, 'labels':y_train}\n",
    "    val   = {'samples':X_val, 'labels':y_val}\n",
    "    test   = {'samples':X_test, 'labels':y_test}\n",
    "    torch.save(train, f'train_{name}.pt')\n",
    "    torch.save(val,f'val_{name}.pt')\n",
    "    torch.save(test,f'test_{name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HAR_data_loader(dataset, batch_size, shuffle, drop_last) :\n",
    "    # datasets \n",
    "    train_dataset = HAR_torch_Dataset(dataset['train']['samples'], dataset['train']['labels'])\n",
    "    val_dataset = HAR_torch_Dataset(dataset['val']['samples'], dataset['val']['labels'])\n",
    "    test_dataset = HAR_torch_Dataset(dataset['test']['samples'], dataset['test']['labels'])\n",
    "    # dataloaders\n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    valid_dl = DataLoader(val_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=10, shuffle=False, drop_last=False)\n",
    "\n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloading \n",
    "data_dir = r'D:\\My TS Datasets\\HAR_Datasets\\HAR\\UCI HAR Dataset'\n",
    "domains_data = []\n",
    "domains_labels = []\n",
    "subject_data= np.loadtxt(f'{data_dir}/train/subject_train.txt')\n",
    "# Samples\n",
    "train_acc_x= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_x_train.txt')\n",
    "train_acc_y= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_y_train.txt')\n",
    "train_acc_z= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_z_train.txt')\n",
    "train_gyro_x= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_x_train.txt')\n",
    "train_gyro_y= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_y_train.txt')\n",
    "train_gyro_z= np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_z_train.txt')\n",
    "train_tot_acc_x= np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_x_train.txt')\n",
    "train_tot_acc_y= np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_y_train.txt')\n",
    "train_tot_acc_z= np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_z_train.txt')\n",
    "\n",
    "test_acc_x= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_x_test.txt')\n",
    "test_acc_y= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_y_test.txt')\n",
    "test_acc_z= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_z_test.txt')\n",
    "test_gyro_x= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_x_test.txt')\n",
    "test_gyro_y= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_y_test.txt')\n",
    "test_gyro_z= np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_z_test.txt')\n",
    "test_tot_acc_x= np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_x_test.txt')\n",
    "test_tot_acc_y= np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_y_test.txt')\n",
    "test_tot_acc_z= np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_z_test.txt')\n",
    "\n",
    "# Stacking channels together data \n",
    "train_data= np.stack((train_acc_x,train_acc_y,train_acc_z,\n",
    "                          train_gyro_x,train_gyro_y,train_gyro_z,\n",
    "                          train_tot_acc_x, train_tot_acc_y,train_tot_acc_z),axis=1)\n",
    "test_data= np.stack((test_acc_x,test_acc_y,test_acc_z,\n",
    "                          test_gyro_x,test_gyro_y,test_gyro_z,\n",
    "                          test_tot_acc_x, test_tot_acc_y,test_tot_acc_z),axis=1)\n",
    "# labels \n",
    "train_labels=  np.loadtxt(f'{data_dir}/train/y_train.txt')\n",
    "train_labels -= np.min(train_labels)\n",
    "test_labels=  np.loadtxt(f'{data_dir}/test/y_test.txt')\n",
    "test_labels -= np.min(test_labels)\n",
    "\n",
    "# different subjects \n",
    "subject_train= np.loadtxt(f'{data_dir}/train/subject_train.txt')\n",
    "subject_test= np.loadtxt(f'{data_dir}/test/subject_test.txt')\n",
    "# select subset i for train and subset j for testing\n",
    "\n",
    "all_subjects_data = np.concatenate((train_data, test_data))\n",
    "all_subjects_labels = np.concatenate((train_labels, test_labels))\n",
    "subject_indices =  np.concatenate((subject_train, subject_test))\n",
    "# arrange the subjects to different domains\n",
    "\n",
    "for i in range(1, 31):\n",
    "    subject = all_subjects_data[np.where(subject_indices==i)]\n",
    "    label = all_subjects_labels[np.where(subject_indices==i)]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(subject, label, test_size=0.3, stratify=label, random_state=1)\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25,  stratify=True, random_state=1)\n",
    "\n",
    "    train = {'samples':X_train, 'labels':y_train}\n",
    "#     val   = {'samples':X_val, 'labels':y_val}\n",
    "    test   = {'samples':X_test, 'labels':y_test}\n",
    "    torch.save(train, f'train_{i}.pt')\n",
    "#     torch.save(val,f'val_{i}.pt')\n",
    "    torch.save(test,f'test_{i}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 46, 43, 43, 41, 49]\n",
      "[19, 19, 19, 19, 18, 21]\n"
     ]
    }
   ],
   "source": [
    "y= y_train.tolist()\n",
    "print([y.count(i) for i in range(6)])\n",
    "y= y_test.tolist()\n",
    "print([y.count(i) for i in range(6)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
